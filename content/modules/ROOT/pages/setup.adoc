= OpenShift AI

Red Hat OpenShift AI (RHOAI) builds on the capabilities of Red Hat OpenShift to provide a single, consistent, enterprise-ready hybrid AI and MLOps platform. It provides tools across the full lifecycle of AI/ML experiments and models including training, serving, monitoring, and managing AI/ML models and AI-enabled applications.

== Login to RHOAI

. Open https://rhods-dashboard-redhat-ods-applications.{openshift_cluster_ingress_domain}[OpenShift AI^] and click on the *Log in with OpenShift* button.

+
image::openshift/openshift-login-page.png[OpenShift Login,80%,80%]

. Click on the *rhsso* button.

+
image::openshift/openshift-rhsso-page.png[Login with rhsso,80%,80%]

. Enter your *username* {user} and *password* {password} for the lab and click on *Sign In*.

+
image::openshift/openshift-userpass-page.png[Login with username and password,80%,80%]

+
You should now see the OpenShift AI dashboard. Click on the *summit-project-{user}* under Data Science Projects in the dashboard.

+
image::openshift/rhoai-dashboard.png[RHOAI Dashboard,100%,100%]

== Configure Data Science Pipelines
Before we can create our OpenShift AI workbench we'll need to configure data science pipelines. Data science pipelines has a dependency on s3 object storage so we've pre installed Minio for you to make setup easier.

. Click on *Configure pipeline server*

+
image::openshift/configure-pipelines.png[Configure pipeline server,100%,100%]

. Enter the following values in the form:
+
*Access key*
+
[source,copy,role=execute]
----
minio
----
+
*Secret key*
+
[source,copy,role=execute]
----
minio123
----
+
*Endpoint*
+
[source,copy,role=execute,subs=attributes]
----
http://minio.summit-project-{user}.svc.cluster.local:9000
----
+
*Region*
+
[source,copy,role=execute]
----
us-east-1
----
+
*Bucket*
+
[source,copy,role=execute]
----
pipelines
----
+
image::openshift/configure-pipelines-form.png[Configure pipeline form,100%,100%]

. Wait until the pipeline server is configured. You should see the link to Import pipelines when it's ready. No need to click on the link. 
+
image::openshift/pipelines-ready.png[Pipelines ready,100%,100%]

== Create the Workbench

. Click on the *Create a workbench* button. 
+
image::openshift/create-workbench.png[Create workbench,100%,100%]

. Enter the following values in the form:
+
*Name*
+
[source,copy,role=execute]
----
summit-workbench
----
+
*Image Selection*
+
Select *Standard Data Science* from the dropdown. Keep the default *Version selection* that is recommended.
+
*Environment Variables* - Click on *Add variable*
+
Select *ConfigMap* as the environment variable type and *Key/value*.
+
*Key*
+
[source,copy,role=execute]
----
minio-url
----
+
*Value*
+
[source,copy,role=execute,subs=attributes]
----
http://minio.summit-project-{user}.svc.cluster.local:9000
----
+
image::openshift/create-workbench-form1.png[Create workbench form,100%,100%]
+
image::openshift/create-workbench-form2.png[Create workbench form,100%,100%]

== Open the Workbench

. Once the workbench status is *Running* click on the *Open* link.

+
image::openshift/rhoai-workbench.png[RHOAI Workbench,100%,100%]

+
NOTE: OpenShift AI out of the box provides Jupyter, VS Code, Cuda, and other popular IDEs as workbench images. You can https://ai-on-openshift.io/odh-rhoai/custom-notebooks/[create custom workbench images ,window=_blank] if a default one doesn't exist.

. You'll need to login to you OpenShift AI workbench. Click on the *rhsso* button.

+
image::openshift/openshift-rhsso-page.png[Login with rhsso,80%,80%]

. Enter your *username* ([$user]) and *password* ([$password]) for the lab and click on *Sign In*.

+
image::openshift/openshift-userpass-page.png[Login with username and password,80%,80%]

. Your JupyterLab workbench should now be open. 

+
image::openshift/rhoai-jupyter.png[JupyterLab workbench,100%,100%]

. We have two Git repositories that we'll need to clone. Click on the Git icon on the left menu and click *Clone a repository*.
+ 
image::openshift/jupyter-git-clone.png[Button to clone a Git repository,100%,100%]

. Clone the following repo. This code contains the Elyra pipeline. 
+
[source,copy,role=execute]
----
https://github.com/jhurlocker/elyra_docling_rh_summit.git 
----
+
image::local/git-clone1.png[Box to clone a Git repository,100%,100%]

+
Clone this repo as well. It contains the code to generate the markdown file and the qna.yaml file.
+
[source,copy,role=execute]
----
https://github.com/jhurlocker/instruct-generate
----
+ 
image::local/git-clone2.png[Box to clone a Git repository,100%,100%]

+
You should now see the *elyra_docling_rh_summit* and *instruct_generate* directories in the file browser.

+
image::openshift/jupyter-cloned-directories.png[Cloned repositories,80%,80%]

//// 
== Deploy a Model

We'll be deploying a quantized version of an LLM so we can run it on a CPU. Quantized LLMs have been converted from high-precision floating-point numbers to lower-precision data types to reduce its size and increase computational speed.

. Go to your *Data Science Project* and click on *Models*. We'll be using the single-model serving platform. Click *Select single-model*. 

+ 
image::openshift/single-server-model.png[Single server model,100%,100%]

. Click on *Deploy model*. 

+ 
image::openshift/deploy-model.png[Single server model,100%,100%]
////
