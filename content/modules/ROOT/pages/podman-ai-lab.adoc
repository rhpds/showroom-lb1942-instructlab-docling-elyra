= Serving a Model with Podman AI Lab

[#overview]
== Podman AI Lab Overview


Podman AI Lab in an extension to Podman Desktop and provides an easy way to work with Large Language Models (LLMs) on your local workstation.

* *Model Catalog & Comparison*: Access a catalog of recipes and a curated list of open-source models for experimentation and comparison.
* *Local Container Integration*: Seamlessly integrate LLMs into your local container workflow, exposing inference APIs.
* *Direct Application Access*: Enable direct access to LLMs from your application containers.
* *Inference Optimization*: Utilize playgrounds to fine-tune inference parameters.
* *Ready-Made Examples*: Leverage recipes for practical, ready-made examples.

==  Start Podman AI Lab

. Open the LB1942: Podman Desktop AI noVNC Web URL and click the *Send Credentials* with the password provided.

+ 
image::local/noVNC-login-page.png[noVNC login prompt,100%,100%]


. Once you are logged in click on *Activities* in the top left corner of the desktop.

+
image::local/rhel9-activities.png[RHEL 9 Activities,100%,100%]

. Click on the *Show Applications* icon.

+
image::local/rhel9-show-apps.png[RHEL 9 Show Applications,100%,100%]

. Click the *Podman Desktop* icon.

+
image::local/rhel9-podman-desktop.png[HEL 9 Podman Desktop,100%,100%]

. Click the *AI Lab* icon on the left menu, then click *Catalog* under MODELS, *Downloaded* under Models, and finally click on the *Create Model Service* icon.

+
image::local/rhel9-podman-ai-1.png[Podman AI downloaded models,100%,100%]

+
NOTE: You can hide the enable GPU support banner by clicking on *Don't display anymore*

. Click the *Create Service* button. 

+
image::local/rhel9-podman-ai-2.png[Podman AI create service,100%,100%]

. Click the *Open service details* button. 

+
image::local/rhel9-podman-ai-3.png[Podman AI service details,100%,100%]

. You now have a quantized LLM deployed to your local VM! Note the inference endpoint URL.

+
We'll use that in our Jupyter notebook when we generate InstructLab markdown and qna.yaml files. 

+
image::local/rhel9-podman-ai-4.png[Inference service details,100%,100%]

== Extra Section

. Click on the *Playgrounds* on the left menu and then click *New Playground*.

+
image::local/rhel9-podman-ai-5.png[Podman AI Lab Playground,100%,100%]

. Click on the *Create Playground* button. You can leave the Playground name blank.

+
image::local/rhel9-podman-ai-6.png[Podman AI Lab New Playground,100%,100%]

. Click on the *playground 1* name.

+
image::local/rhel9-podman-ai-7.png[Podman AI Lab New Playground,100%,100%]

. This screen allows you to interact with the LLM deployed on Podman desktop. Type in some prompts to test it out!

+
image::local/rhel9-podman-ai-8.png[Podman AI Playground UI,100%,100%]

In this lab, you will be installing software into the container 
image running as an interactive application. To do this you will
need `yum`, but do not need `systemd` for managing services within the
container environment.  For that reason, you will be using the *Standard*
UBI image (as opposed to the Minimal or Multi-service images).

Using the "buildah from" command will download and meld the container image. This particular image we are using is the Red Hat Universal Base Image or UBI. From the ourput of the command, you will notice that we are pulling down the latest one, which is for RHEL 9. 

. Execute the  download the Standard UBI
image from Red Hat's registry.

+
[source,sh,role=execute]
----
buildah from registry.access.redhat.com/ubi9/ubi
----



+
[source,bash]
----
buildah run ubi-working-container -- yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm
----


. You can verify that the above command did not install the RPM on the host system.

+
[source,bash]
----
rpm -q epel-release
----

NOTE: If your repository configurations are not distributed as an RPM, but instead as
individual `.repo` files, you could use the `buildah copy` command to copy
files from the host operating system into the container image.  You will see
an example of using `buildah copy` later in this lab.

[#serve-model]
== Start Service

TODO - hands on instructions on serving the model and sending a simple request.

+
[source,bash]
----
buildah run ubi-working-container -- yum -y install moon-buggy
----


== Committing the Container Image

. At this point, the container is configured.  It is time to transition from a
working container into a committed image.  In the command below, you will use
the `buildah` command to commit the working container to an image called:
`moon-buggy`.

+
[source,bash]
----
buildah commit ubi-working-container moon-buggy
----

+
. The output of `podman image list` should confirm the image was created.

+
[source,bash]
----
podman image list
----


== Deploy the Container

Now the software has been installed and a new container image created.  It is
time to spawn a runtime of the container image and validate the software.  The
software we are using is a command line command.  

. When you `run` the container,
it will be in interactive (`-it`) mode, based on the `moon-buggy` container
image and the command run interactively will be `/usr/bin/moon-buggy`.

+
[source,bash]
----
podman run -it moon-buggy /usr/bin/moon-buggy
----

+
[source,textinfo]
----

<<< OUTPUT ABRIDGED >>>
               MM     MM   OOOOO    OOOOO   NN     N
               M M   M M  O     O  O     O  N N    N
               M  M M  M  O     O  O     O  N  N   N
               M   M   M  O     O  O     O  N   N  N
               M       M  O     O  O     O  N    N N
               M       M   OOOOO    OOOOO   N     NN

                     BBBBBB   U     U   GGGGG    GGGGG   Y     Y
                     B     B  U     U  G     G  G     G   Y   Y
                     BBBBBB   U     U  G        G          Y Y
                     B     B  U     U  G   GGG  G   GGG     Y
                     B     B  U     U  G     G  G     G    Y
                     BBBBBB    UUUUU    GGGGG    GGGGG   YY

<<< OUTPUT ABRIDGED >>>
----

. You can now play the Moon Buggy game, which is a text-based version of the
popular Moon Patrol.  When you are finished, use the `q` command to quit the
game, which will terminate the container.

+
Alternatively, you can use `podman` to kill the running container from
*Terminal 2*.

+
[source,bash]
----
podman kill $(podman ps | grep -v CONTAINER | cut -f1 -d" " )
----
