= OpenShift Pipelines and Minio

In the last section we manually ran the Elyra Data Science pipeline from a Jupyter notebook, but what if we want to run this pipeline anytime we get new data? For that we can use OpenShift event listeners and OpenShift pipelines to automatically trigger new pipeline runs when data is uploaded to our s3 bucket.

== Event Listeners[[ocppipelines]]

. Login to your OpenShift console.

+
image::openshift/openshift-login-page.png[OpenShift login screen,80%,80%]

. Select *rhsso*. 

+
image::openshift/openshift-rhsso-page.png[OpenShift rhsso screen,80%,80%]

. Login with your username and password.

+
image::openshift/openshift-userpass-page.png[OpenShift user/pass screen,80%,80%]

. You should see the OpenShift Web console with the *summit_project* 

+
image::openshift/ocp-project.png[OpenShift project,100%,100%]

. Scroll down and expand the *Pipelines* section on the left menu. Select the *Triggers*. Note that we have an event listener named *simple-pipeline-listener* setup.

+
image::openshift/ocp-main-triggers.png[OpenShift triggers,100%,100%]

. Click on *Trigger Templates* -> *simple-template* -> *YAML*. The event listener is configured to run this trigger template when activated. This template runs our *start-dsp-pipeline*. 

+
image::openshift/ocp-trigger-template.png[OpenShift trigger template,100%,100%]

. Click on *Pipelines*. You should see the *start-dsp-pipeline*. This OpenShift pipeline is responsible for starting our OpenShift AI Data Science Pipeline.

+
image::openshift/ocp-pipeline.png[OpenShift pipeline,100%,100%]

== Minio[[minio]]

We're using Mino for our s3 buckets.

. Get the route for Mino by going to the OpenShift web console and make sure you change your Project to *All Projects*. Select *Networking* -> *Routes*. Type _minio-ui_ into the search bar and click the *Location* URL to open the Minio web console.

+
image::openshift/minio-route.png[Mino route,100%,100%]

. Login to the Mino web console with the user *minio* and password *minio123*.

+
image::openshift/minio-login.png[Mino login page,80%,80%]

. We'll create an event destination so that our event listener can trigger our OpenShift pipeline whenever we upload files to the s3 bucket.

+
Click on *Events* on the left menu and click on the *Add event destination* button.

+
image::openshift/minio-add-event-destination.png[Mino add event destination,100%,100%]

. Scroll down until you see the webhook function. Click on *Webhook*. 

+
image::openshift/minio-webhook-event.png[Mino webhook event,100%,100%]

. We'll need to get our OpenShift token before we can completely configure the event destination in Minio. 

+
image::openshift/ocp-token.png[OpenShift token,100%,100%]

. Enter in _dsp-event-listener_ for the *Identifier*, the event listener route URL for the *Endpoint*, and your OpenShift token in the *Auth Token* field.

+
Click *Save Event Destination* 

+
image::openshift/minio-event-destination.png[Mino configure event destination,100%,100%]

. Under Administration on the left hand menu click *Buckets*. Open the *upload-files* bucket and click *Events* for the upload-files bucket. Click on *Subscribe to Event*.

+
image::openshift/minio-subscribe-to-events.png[Mino subscribe to events,100%,100%]

. Select the *dsp-event-listener* webhook from the dropdown. Select the *PUT - Object Uploaded* event and click *Save*.

+
image::openshift/minio-subscribe-to-bucket-events.png[Mino subscribe to events,100%,100%]

Your s3 bucket is now configured to send an event to your OpenShift event listener every time a file is uploaded. In the next section we'll upload a file and see how it runs our pipelines!